{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9KOif0Zs3jL",
        "outputId": "f915ae44-eb29-4be7-a8df-2e060ef1718c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.253-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.253-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.253 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y clip\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyGiYR9ztJHZ",
        "outputId": "ee537fd9-0339-4587-c2d5-a298b7311dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: clip 1.0\n",
            "Uninstalling clip-1.0:\n",
            "  Successfully uninstalled clip-1.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-sb69q8jk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-sb69q8jk\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cpu)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=d91778fcfa4c5de93aa584379ebc77f8aa207161a14e9ec7e55184d95ba9634b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rubt10h7/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y clip\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuFNJerQkwbM",
        "outputId": "48fe886b-b24d-413a-caa3-b2e3e9139ab5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: clip 1.0\n",
            "Uninstalling clip-1.0:\n",
            "  Successfully uninstalled clip-1.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-premn2g7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-premn2g7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cpu)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=c4ab84e68a3b3af97eec598171d9418625116a2b1a4a22ea80e9db17fd5f23ae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7z1g1_gd/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import clip\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "2bJXH16qekkT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OVODDetector:\n",
        "    def __init__(self, detector_weights, clip_model_name=\"ViT-B/32\"):\n",
        "        \"\"\"Initialize OVOD Detector\"\"\"\n",
        "        # Automatically detect available device\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load YOLO detector\n",
        "        print(f\"Loading YOLO model from {detector_weights}...\")\n",
        "        self.detector = YOLO(detector_weights)\n",
        "\n",
        "        # Load CLIP for open-vocabulary classification\n",
        "        print(f\"Loading CLIP model {clip_model_name}...\")\n",
        "        try:\n",
        "            self.clip_model, self.clip_preprocess = clip.load(clip_model_name, device=self.device)\n",
        "            self.clip_model.eval()\n",
        "            print(\" CLIP loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading CLIP: {e}\")\n",
        "            print(\"Trying reinstallation...\")\n",
        "            os.system(\"pip install git+https://github.com/openai/CLIP.git\")\n",
        "            import importlib\n",
        "            importlib.reload(clip)\n",
        "            self.clip_model, self.clip_preprocess = clip.load(clip_model_name, device=self.device)\n",
        "            self.clip_model.eval()\n",
        "\n",
        "        # YOLO known classes\n",
        "        self.yolo_classes = [\n",
        "            'backpack', 'banner', 'carton', 'chair', 'desk', 'door',\n",
        "            'fire extinguisher', 'light bulb', 'miscellaneous', 'motorcycle',\n",
        "            'person', 'pillar', 'staircase', 'step', 'table', 'tiled floor',\n",
        "            'trash bin', 'tree', 'window', 'windows frame'\n",
        "        ]\n",
        "\n",
        "        # New classes (not in training set)\n",
        "        self.unseen_classes = [\n",
        "            'box', 'broom', 'cabinet', 'computer', 'fan', 'robot',\n",
        "            'umbrella', 'laptop', 'book', 'bottle', 'keyboard', 'mouse'\n",
        "        ]\n",
        "\n",
        "        # Pre-compute CLIP text features\n",
        "        print(\"Pre-computing CLIP text features...\")\n",
        "        self.yolo_text_features = self._encode_text_prompts(self.yolo_classes)\n",
        "        self.unseen_text_features = self._encode_text_prompts(self.unseen_classes)\n",
        "\n",
        "        print(f\" YOLO can detect: {len(self.yolo_classes)} classes\")\n",
        "        print(f\" OVOD added: {len(self.unseen_classes)} new classes\")\n",
        "\n",
        "\n",
        "    def _encode_text_prompts(self, class_names):\n",
        "        \"\"\"Encode class names to CLIP text features\"\"\"\n",
        "        templates = [\n",
        "            \"a photo of a {}\",\n",
        "            \"a picture of a {}\",\n",
        "            \"an image of a {}\",\n",
        "            \"a {} in the scene\",\n",
        "            \"a {} object\",\n",
        "            \"{}\"\n",
        "        ]\n",
        "\n",
        "        text_inputs = []\n",
        "        for cls in class_names:\n",
        "            for template in templates:\n",
        "                text_inputs.append(template.format(cls))\n",
        "\n",
        "        text_tokens = clip.tokenize(text_inputs).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            text_features = text_features.reshape(len(class_names), len(templates), -1)\n",
        "            text_features = text_features.mean(dim=1)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def _non_maximum_suppression(self, detections, iou_threshold=0.5):\n",
        "\n",
        "        if len(detections) <= 1:\n",
        "            return detections\n",
        "\n",
        "        # Sort by confidence\n",
        "        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "        filtered = []\n",
        "        while detections:\n",
        "            best = detections.pop(0)\n",
        "            filtered.append(best)\n",
        "\n",
        "            to_remove = []\n",
        "            for i, det in enumerate(detections):\n",
        "                iou = self._calculate_iou(best['bbox'], det['bbox'])\n",
        "                if iou > iou_threshold:\n",
        "                    to_remove.append(i)\n",
        "\n",
        "            for idx in reversed(to_remove):\n",
        "                detections.pop(idx)\n",
        "\n",
        "        return filtered\n",
        "\n",
        "    def _calculate_iou(self, box1, box2):\n",
        "\n",
        "        x1 = max(box1[0], box2[0])\n",
        "        y1 = max(box1[1], box2[1])\n",
        "        x2 = min(box1[2], box2[2])\n",
        "        y2 = min(box1[3], box2[3])\n",
        "\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            return 0.0\n",
        "\n",
        "        intersection = (x2 - x1) * (y2 - y1)\n",
        "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "        union = area1 + area2 - intersection\n",
        "\n",
        "        return intersection / union if union > 0 else 0\n",
        "\n",
        "    def yolo_detection(self, image, confidence_thresh=0.3):\n",
        "        \"\"\"YOLO detection for known classes\"\"\"\n",
        "        results = self.detector(image, verbose=False, conf=confidence_thresh)\n",
        "        detections = []\n",
        "\n",
        "        for result in results:\n",
        "            if result.boxes is not None:\n",
        "                for i, box in enumerate(result.boxes):\n",
        "                    if box.conf.item() > confidence_thresh:\n",
        "                        detections.append({\n",
        "                            'bbox': box.xyxy[0].cpu().numpy().tolist(),\n",
        "                            'confidence': box.conf.item(),\n",
        "                            'class_id': int(box.cls.item()),\n",
        "                            'class_name': self.yolo_classes[int(box.cls.item())],\n",
        "                            'type': 'known',\n",
        "                            'source': 'yolo'\n",
        "                        })\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def clip_based_detection(self, image, confidence_thresh=0.6, grid_size=128, max_proposals=100):\n",
        "        \"\"\"CLIP-based detection for new classes\"\"\"\n",
        "        h, w = image.shape[:2]\n",
        "        detections = []\n",
        "\n",
        "        # Generate region proposals\n",
        "        proposals = []\n",
        "        for y in range(0, h - grid_size, grid_size):\n",
        "            for x in range(0, w - grid_size, grid_size):\n",
        "                for scale in [1, 1.5]:\n",
        "                    size = int(grid_size * scale)\n",
        "                    if x + size <= w and y + size <= h:\n",
        "                        proposals.append([x, y, x + size, y + size])\n",
        "\n",
        "        # Limit total proposals\n",
        "        proposals = proposals[:max_proposals]\n",
        "\n",
        "        # Batch processing\n",
        "        batch_size = 16\n",
        "        for i in range(0, len(proposals), batch_size):\n",
        "            batch_proposals = proposals[i:i+batch_size]\n",
        "            batch_images = []\n",
        "            valid_indices = []\n",
        "\n",
        "            for j, (x, y, x2, y2) in enumerate(batch_proposals):\n",
        "                region = image[y:y2, x:x2]\n",
        "                if region.size == 0:\n",
        "                    continue\n",
        "\n",
        "                pil_image = Image.fromarray(cv2.cvtColor(region, cv2.COLOR_BGR2RGB))\n",
        "                preprocessed = self.clip_preprocess(pil_image).unsqueeze(0)\n",
        "                batch_images.append(preprocessed)\n",
        "                valid_indices.append(j)\n",
        "\n",
        "            if not batch_images:\n",
        "                continue\n",
        "\n",
        "            # Batch inference\n",
        "            batch_tensor = torch.cat(batch_images).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip_model.encode_image(batch_tensor)\n",
        "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                similarities = image_features @ self.unseen_text_features.T\n",
        "                max_sims, best_indices = similarities.max(dim=1)\n",
        "\n",
        "            # Collect results\n",
        "            for k, (sim, idx) in enumerate(zip(max_sims, best_indices)):\n",
        "                sim_value = sim.item()\n",
        "                if sim_value > confidence_thresh:\n",
        "                    original_idx = valid_indices[k]\n",
        "                    x, y, x2, y2 = batch_proposals[original_idx]\n",
        "\n",
        "                    # Calculate bounding box quality\n",
        "                    area = (x2 - x) * (y2 - y)\n",
        "                    img_area = h * w\n",
        "                    area_ratio = area / img_area\n",
        "\n",
        "                    if 0.01 < area_ratio < 0.5:\n",
        "                        detections.append({\n",
        "                            'bbox': [x, y, x2, y2],\n",
        "                            'confidence': sim_value,\n",
        "                            'class_id': idx.item(),\n",
        "                            'class_name': self.unseen_classes[idx.item()],\n",
        "                            'type': 'unseen',\n",
        "                            'source': 'clip'\n",
        "                        })\n",
        "\n",
        "        # Apply NMS\n",
        "        if len(detections) > 0:\n",
        "            detections = self._non_maximum_suppression(detections, iou_threshold=0.3)\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def predict_ovod(self, image_path, confidence_thresh=0.3, detect_unseen=True):\n",
        "\n",
        "        # Read image\n",
        "        if isinstance(image_path, str):\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Cannot read image: {image_path}\")\n",
        "        else:\n",
        "            image = image_path\n",
        "\n",
        "        all_detections = []\n",
        "\n",
        "        # 1. YOLO detection for known classes\n",
        "\n",
        "        yolo_detections = self.yolo_detection(image, confidence_thresh)\n",
        "        print(f\"YOLO found {len(yolo_detections)} known objects\")\n",
        "\n",
        "        # Filter YOLO results\n",
        "        filtered_yolo = []\n",
        "        h, w = image.shape[:2]\n",
        "        img_area = h * w\n",
        "\n",
        "        for det in yolo_detections:\n",
        "            x1, y1, x2, y2 = det['bbox']\n",
        "            area = (x2 - x1) * (y2 - y1)\n",
        "            area_ratio = area / img_area\n",
        "\n",
        "            if 0.002 < area_ratio < 0.8:\n",
        "                filtered_yolo.append(det)\n",
        "\n",
        "        print(f\"Filtered YOLO detections: {len(filtered_yolo)}\")\n",
        "        all_detections.extend(filtered_yolo)\n",
        "\n",
        "        # 2. CLIP detection for new classes\n",
        "        if detect_unseen and len(self.unseen_classes) > 0:\n",
        "\n",
        "            unseen_detections = self.clip_based_detection(\n",
        "                image,\n",
        "                confidence_thresh=0.25,\n",
        "\n",
        "                grid_size=128,\n",
        "                max_proposals=150\n",
        "            )\n",
        "\n",
        "            # Filter overlapping with YOLO detections\n",
        "            final_unseen = []\n",
        "            for unseen in unseen_detections:\n",
        "                overlap = False\n",
        "                for seen in all_detections:\n",
        "                    iou = self._calculate_iou(unseen['bbox'], seen['bbox'])\n",
        "                    if iou > 0.2:\n",
        "                        overlap = True\n",
        "                        break\n",
        "\n",
        "                if not overlap:\n",
        "                    final_unseen.append(unseen)\n",
        "\n",
        "            print(f\"CLIP found {len(final_unseen)} new objects\")\n",
        "            all_detections.extend(final_unseen)\n",
        "\n",
        "        # 3. Final NMS\n",
        "        if len(all_detections) > 1:\n",
        "            all_detections = self._non_maximum_suppression(all_detections, iou_threshold=0.5)\n",
        "\n",
        "        # 4. Limit total detections\n",
        "        if len(all_detections) > 30:\n",
        "            all_detections = sorted(all_detections, key=lambda x: x['confidence'], reverse=True)\n",
        "            all_detections = all_detections[:30]\n",
        "            print(f\"Limiting total detections to 30\")\n",
        "\n",
        "        return all_detections\n",
        "\n",
        "    def visualize_results(self, image_path, detections, output_path=None):\n",
        "        \"\"\"Visualize results\"\"\"\n",
        "        if isinstance(image_path, str):\n",
        "            image = cv2.imread(image_path)\n",
        "        else:\n",
        "            image = image_path.copy()\n",
        "\n",
        "        # Color definitions\n",
        "        colors = {\n",
        "            'known': (0, 255, 0),    # Green - YOLO known classes\n",
        "            'unseen': (0, 0, 255)    # Red - CLIP new classes\n",
        "        }\n",
        "\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2 = map(int, det['bbox'])\n",
        "            det_type = det.get('type', 'known')\n",
        "            color = colors.get(det_type, (255, 255, 255))\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "            # Prepare label\n",
        "            label = f\"{det['class_name']}: {det['confidence']:.2f}\"\n",
        "            if det_type == 'unseen':\n",
        "                label = f\"{det['class_name']} (new): {det['confidence']:.2f}\"\n",
        "\n",
        "            # Draw label\n",
        "            (text_width, text_height), baseline = cv2.getTextSize(\n",
        "                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "\n",
        "            cv2.rectangle(image, (x1, y1 - text_height - baseline - 5),\n",
        "                         (x1 + text_width, y1), color, -1)\n",
        "\n",
        "            cv2.putText(image, label, (x1, y1 - baseline - 5),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "\n",
        "        if output_path:\n",
        "            cv2.imwrite(output_path, image)\n",
        "            print(f\"Result saved to {output_path}\")\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "id": "q85AmHHekWVy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ovod_performance(detector, dataset_path, output_dir=\"/content/drive/MyDrive/yolo8/results\"):\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    seen_classes = [\n",
        "        'backpack', 'chair', 'desk', 'door', 'person', 'pillar',\n",
        "        'staircase', 'table', 'tiled floor', 'trash bin', 'tree',\n",
        "        'window', 'banner', 'fire extinguisher'\n",
        "    ]\n",
        "\n",
        "    unseen_classes = [\n",
        "        'box', 'broom', 'cabinet', 'computer', 'fan', 'robot',\n",
        "        'umbrella', 'laptop', 'book', 'bottle', 'keyboard', 'mouse'\n",
        "    ]\n",
        "\n",
        "    # Find validation images\n",
        "    val_path = Path(dataset_path) / \"valid\" / \"images\"\n",
        "    if not val_path.exists():\n",
        "        val_path = Path(dataset_path) / \"valid\"\n",
        "\n",
        "    image_files = list(val_path.glob(\"*.jpg\")) + list(val_path.glob(\"*.png\"))\n",
        "    print(f\"Found {len(image_files)} validation images\")\n",
        "\n",
        "    if len(image_files) == 0:\n",
        "        print(f\"Error: No images found in {val_path}\")\n",
        "        return None\n",
        "\n",
        "    # Run inference and collect results\n",
        "    print(\"Running inference...\")\n",
        "\n",
        "    all_results = []\n",
        "    for img_path in image_files[:50]:  # Only test first 50 images for speed\n",
        "        try:\n",
        "            try:\n",
        "                detections = detector.predict_ovod(\n",
        "                    str(img_path),\n",
        "                    confidence_thresh=0.3,\n",
        "                    detect_unseen=True,\n",
        "\n",
        "                )\n",
        "            except TypeError as e:\n",
        "                if \"clip_confidence\" in str(e):\n",
        "\n",
        "                    detections = detector.predict_ovod(\n",
        "                        str(img_path),\n",
        "                        confidence_thresh=0.3,\n",
        "                        detect_unseen=True\n",
        "                    )\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "            for det in detections:\n",
        "                result = {\n",
        "                    'image_id': img_path.stem,\n",
        "                    'category': det['class_name'],\n",
        "                    'bbox': det['bbox'],\n",
        "                    'score': det['confidence'],\n",
        "                    'type': det['type']  # 'known' or 'unseen'\n",
        "                }\n",
        "                all_results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {img_path.name}: {e}\")\n",
        "\n",
        "    print(f\"Collected {len(all_results)} detections\")\n",
        "\n",
        "    # Analyze results\n",
        "    seen_detections = [r for r in all_results if r['type'] == 'known']\n",
        "    unseen_detections = [r for r in all_results if r['type'] == 'unseen']\n",
        "\n",
        "\n",
        "    seen_quality = calculate_quality_score(seen_detections, is_seen=True)\n",
        "    unseen_quality = calculate_quality_score(unseen_detections, is_seen=False)\n",
        "\n",
        "    # Compile results\n",
        "    results = {\n",
        "        'seen_mAP50': seen_quality,\n",
        "        'unseen_mAP50': unseen_quality,\n",
        "        'harmonic_mean': calculate_harmonic_mean(seen_quality, unseen_quality),\n",
        "        'statistics': {\n",
        "            'total_images': len(image_files),\n",
        "            'total_detections': len(all_results),\n",
        "            'seen_detections': len(seen_detections),\n",
        "            'unseen_detections': len(unseen_detections),\n",
        "            'seen_classes_detected': count_unique_classes(seen_detections),\n",
        "            'unseen_classes_detected': count_unique_classes(unseen_detections),\n",
        "            'avg_confidence_seen': np.mean([d['score'] for d in seen_detections]) if seen_detections else 0,\n",
        "            'avg_confidence_unseen': np.mean([d['score'] for d in unseen_detections]) if unseen_detections else 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add confidence statistics for unseen detections\n",
        "    if unseen_detections:\n",
        "        unseen_scores = [d['score'] for d in unseen_detections]\n",
        "        results['unseen_confidence_stats'] = {\n",
        "            'mean': float(np.mean(unseen_scores)),\n",
        "            'median': float(np.median(unseen_scores)),\n",
        "            'std': float(np.std(unseen_scores)),\n",
        "            'min': float(np.min(unseen_scores)),\n",
        "            'max': float(np.max(unseen_scores)),\n",
        "            'above_0.3': len([s for s in unseen_scores if s >= 0.3]) / len(unseen_scores),\n",
        "            'above_0.4': len([s for s in unseen_scores if s >= 0.4]) / len(unseen_scores)\n",
        "        }\n",
        "\n",
        "    # Save results\n",
        "    results_file = os.path.join(output_dir, 'evaluation_results.json')\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"OVOD EVALUATION RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Seen Classes Quality Score: {results['seen_mAP50']:.3f}\")\n",
        "    print(f\"Unseen Classes Quality Score: {results['unseen_mAP50']:.3f}\")\n",
        "    print(f\"Harmonic Mean: {results['harmonic_mean']:.3f}\")\n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"  Images processed: {results['statistics']['total_images']}\")\n",
        "    print(f\"  Total detections: {results['statistics']['total_detections']}\")\n",
        "    print(f\"    - Seen: {results['statistics']['seen_detections']}\")\n",
        "    print(f\"    - Unseen: {results['statistics']['unseen_detections']}\")\n",
        "    print(f\"  Unique seen classes detected: {results['statistics']['seen_classes_detected']}\")\n",
        "    print(f\"  Unique unseen classes detected: {results['statistics']['unseen_classes_detected']}\")\n",
        "    print(f\"  Avg confidence (seen): {results['statistics']['avg_confidence_seen']:.3f}\")\n",
        "    print(f\"  Avg confidence (unseen): {results['statistics']['avg_confidence_unseen']:.3f}\")\n",
        "\n",
        "    # Print confidence statistics for unseen detections\n",
        "    if 'unseen_confidence_stats' in results:\n",
        "        print(f\"\\nUnseen Detections Confidence Analysis:\")\n",
        "        stats = results['unseen_confidence_stats']\n",
        "        print(f\"  Mean confidence: {stats['mean']:.3f}\")\n",
        "        print(f\"  Median confidence: {stats['median']:.3f}\")\n",
        "        print(f\"  Range: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
        "        print(f\"  Above 0.3: {stats['above_0.3']:.1%}\")\n",
        "        print(f\"  Above 0.4: {stats['above_0.4']:.1%}\")\n",
        "\n",
        "    # Print detected classes\n",
        "    if unseen_detections:\n",
        "        print(f\"\\nDetected unseen classes:\")\n",
        "        class_counts = {}\n",
        "        for det in unseen_detections:\n",
        "            class_counts[det['category']] = class_counts.get(det['category'], 0) + 1\n",
        "\n",
        "        for cls, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {cls}: {count} detections\")\n",
        "\n",
        "    print(f\"\\nNote: These are quality scores, not true mAP.\")\n",
        "    print(f\"True mAP requires ground truth annotations for comparison.\")\n",
        "    print(f\"Results saved to: {results_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def calculate_quality_score(detections, is_seen=True):\n",
        "\n",
        "    if len(detections) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    scores = [d['score'] for d in detections]\n",
        "\n",
        "    if is_seen:\n",
        "        # For seen classes: proportion of high confidence detections\n",
        "        high_conf = [s for s in scores if s >= 0.5]\n",
        "        if len(scores) == 0:\n",
        "            return 0.0\n",
        "        quality = len(high_conf) / len(scores) * 0.7 + np.mean(scores) * 0.3\n",
        "    else:\n",
        "        # For unseen classes: consider confidence distribution and diversity\n",
        "        # 1. Average confidence (40% weight)\n",
        "        mean_confidence = np.mean(scores)\n",
        "\n",
        "        # 2. High confidence ratio (30% weight)\n",
        "        high_conf_ratio = len([s for s in scores if s >= 0.3]) / len(scores)\n",
        "\n",
        "        # 3. Class diversity (30% weight)\n",
        "        unique_classes = len(set([d['category'] for d in detections]))\n",
        "        diversity_score = min(unique_classes / 5.0, 1.0)\n",
        "        quality = (mean_confidence * 0.4 + high_conf_ratio * 0.3 + diversity_score * 0.3)\n",
        "\n",
        "    return float(min(quality, 1.0))  # Ensure doesn't exceed 1\n",
        "\n",
        "\n",
        "def calculate_harmonic_mean(seen_score, unseen_score):\n",
        "    \"\"\"Calculate harmonic mean\"\"\"\n",
        "    if seen_score == 0 or unseen_score == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return 2 * seen_score * unseen_score / (seen_score + unseen_score)\n",
        "\n",
        "\n",
        "def count_unique_classes(detections):\n",
        "    \"\"\"Count unique detected classes\"\"\"\n",
        "    unique_classes = set()\n",
        "    for det in detections:\n",
        "        unique_classes.add(det['category'])\n",
        "    return len(unique_classes)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Initialize detector\n",
        "    detector = OVODDetector(\n",
        "        detector_weights='/content/drive/MyDrive/yolo8/models/base_detector_seen_classes/weights/best.pt',\n",
        "        clip_model_name=\"ViT-B/32\"\n",
        "    )\n",
        "\n",
        "    # Run evaluation\n",
        "    results = evaluate_ovod_performance(\n",
        "        detector,\n",
        "        '/content/drive/MyDrive/ovod_ws/data/mu-cps-coco',\n",
        "        '/content/drive/MyDrive/yolo8/results'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPilUxr_qrA4",
        "outputId": "8f6237eb-bff3-4b08-f90a-80565cb5d7a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading YOLO model from /content/drive/MyDrive/yolo8/models/base_detector_seen_classes/weights/best.pt...\n",
            "Loading CLIP model ViT-B/32...\n",
            " CLIP loaded successfully\n",
            "Pre-computing CLIP text features...\n",
            " YOLO can detect: 20 classes\n",
            " OVOD added: 12 new classes\n",
            "Found 148 validation images\n",
            "Running inference...\n",
            "YOLO found 6 known objects\n",
            "Filtered YOLO detections: 6\n",
            "CLIP found 4 new objects\n",
            "YOLO found 1 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 11 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 8 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 3 new objects\n",
            "YOLO found 1 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 10 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 9 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 11 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 9 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 12 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 13 new objects\n",
            "YOLO found 5 known objects\n",
            "Filtered YOLO detections: 5\n",
            "CLIP found 14 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 9 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 5 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 11 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 13 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 11 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 8 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 10 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 9 new objects\n",
            "YOLO found 5 known objects\n",
            "Filtered YOLO detections: 5\n",
            "CLIP found 11 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 7 new objects\n",
            "YOLO found 7 known objects\n",
            "Filtered YOLO detections: 7\n",
            "CLIP found 5 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 6 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 7 new objects\n",
            "YOLO found 5 known objects\n",
            "Filtered YOLO detections: 5\n",
            "CLIP found 8 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 11 new objects\n",
            "YOLO found 1 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 12 new objects\n",
            "YOLO found 5 known objects\n",
            "Filtered YOLO detections: 5\n",
            "CLIP found 7 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 8 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 13 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 11 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 12 new objects\n",
            "YOLO found 1 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 12 new objects\n",
            "YOLO found 1 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 12 new objects\n",
            "YOLO found 1 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 8 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 5 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 6 new objects\n",
            "YOLO found 1 known objects\n",
            "Filtered YOLO detections: 1\n",
            "CLIP found 11 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 9 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 10 new objects\n",
            "YOLO found 5 known objects\n",
            "Filtered YOLO detections: 5\n",
            "CLIP found 5 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 5 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 6 new objects\n",
            "YOLO found 5 known objects\n",
            "Filtered YOLO detections: 5\n",
            "CLIP found 3 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 5 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 9 new objects\n",
            "YOLO found 3 known objects\n",
            "Filtered YOLO detections: 3\n",
            "CLIP found 5 new objects\n",
            "YOLO found 2 known objects\n",
            "Filtered YOLO detections: 2\n",
            "CLIP found 5 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 5 new objects\n",
            "YOLO found 4 known objects\n",
            "Filtered YOLO detections: 4\n",
            "CLIP found 3 new objects\n",
            "Collected 566 detections\n",
            "\n",
            "==================================================\n",
            "OVOD EVALUATION RESULTS\n",
            "==================================================\n",
            "Seen Classes Quality Score: 0.733\n",
            "Unseen Classes Quality Score: 0.406\n",
            "Harmonic Mean: 0.523\n",
            "\n",
            "Statistics:\n",
            "  Images processed: 148\n",
            "  Total detections: 566\n",
            "    - Seen: 144\n",
            "    - Unseen: 422\n",
            "  Unique seen classes detected: 9\n",
            "  Unique unseen classes detected: 11\n",
            "  Avg confidence (seen): 0.692\n",
            "  Avg confidence (unseen): 0.264\n",
            "\n",
            "Unseen Detections Confidence Analysis:\n",
            "  Mean confidence: 0.264\n",
            "  Median confidence: 0.262\n",
            "  Range: [0.250, 0.324]\n",
            "  Above 0.3: 0.2%\n",
            "  Above 0.4: 0.0%\n",
            "\n",
            "Detected unseen classes:\n",
            "  cabinet: 186 detections\n",
            "  broom: 144 detections\n",
            "  box: 23 detections\n",
            "  robot: 21 detections\n",
            "  fan: 13 detections\n",
            "  umbrella: 12 detections\n",
            "  bottle: 7 detections\n",
            "  mouse: 5 detections\n",
            "  book: 5 detections\n",
            "  laptop: 4 detections\n",
            "  computer: 2 detections\n",
            "\n",
            "Note: These are quality scores, not true mAP.\n",
            "True mAP requires ground truth annotations for comparison.\n",
            "Results saved to: /content/drive/MyDrive/yolo8/results/evaluation_results.json\n"
          ]
        }
      ]
    }
  ]
}